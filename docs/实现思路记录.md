# 实现思路记录

本文档记录实现中的思路，也可视作开发任务的进度表

## 前向反向

用一个全局的Graph去按照forward的顺序存储所有**算子**

算子(Op类)

grad_func:记录自己的导数

self.last: list     记录所有输入

bp时调用Graph中的倒序去挨个bp

即对每个算子的所有last去进行调用grad_func

至于上下游梯度的传递，我们在Tensor中添加一个father_Op

表示是哪个Op实例产生的这个Tensor

## 线性层(nn)

先实现矩阵乘法

用weight矩阵和bias矩阵组装一下实现forward

### broadcast处理

把广播理解成全连接层那种把一个变量同时给多个算子，bp时需把broadcast部分的导数累计求和

![a23302f2062b4298a44ff00275feae7](https://typorasyt.oss-cn-nanjing.aliyuncs.com/202408191025655.jpg)

(上图为邢存远学长回复)

到具体代码，forward中出现的broadcast不作处理。

bp中检测到，就按照broadcast的逆流程进行压缩

```
if a.ndim!=b.dim  #make sure a.ndim>b.ndim
	把b.dim中大小为1的维度找到，并在a中的对应维度进行压缩(np.sum,keepdim=True)
	把a.dim前面多出的那一段压缩掉(理论上都是维度大小都是1，用sum还是mean不太重要)
```

## loss func

重载OP，可视为：

1.bp无需上游grad 

2.grad_func 无需传入输入

的特殊Op

## optimizer

BGD，SGD(牛魔pytorch的sgd指的不是随机梯度下降，感觉是某种梯度下降的混合体)，AdaGrad，Adam实现后进行简单比较(收敛轮数)

测试思路：调用pytorch进行收敛速度绘图和比较，尝试分析

## nn_module

完成func.py函数里面的函数的torch支持

这里注意，有很多算子的求导相当复杂

比如softmax，一个n维的向量，softmax后还是个n维，求导结果tmd 是个n*n矩阵，而且还有dim这个参数，相当恶心。

故比较复杂的算子我们采用**纯forward**形式，即在forward过程中调用比较简单的**基础算子**，这些算子会自动进入计算图，因此我们无需管理其反向传播。

因此我们对nn.module里面的东西统一采用这种形式进行编码。(与torch中一致)

(这里注意一下，我们的func.py里面存储的东西是不影响计算图的，而torch.nn.functional里面存的东西是会影响的)
