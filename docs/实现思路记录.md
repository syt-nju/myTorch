# 实现思路记录

本文档记录实现中的思路，也可视作开发任务的进度表

## 前向反向

用一个全局的Graph去按照forward的顺序存储所有**算子**

算子(Op类)

grad_func:记录自己的导数

self.last: list     记录所有输入

bp时调用Graph中的倒序去挨个bp

即对每个算子的所有last去进行调用grad_func

至于上下游梯度的传递，我们在Tensor中添加一个father_Op

表示是哪个Op实例产生的这个Tensor

## 线性层(nn)

先实现矩阵乘法

用weight矩阵和bias矩阵组装一下实现forward

### broadcast处理

把广播理解成全连接层那种把一个变量同时给多个算子，bp时需把broadcast部分的导数累计求和

![a23302f2062b4298a44ff00275feae7](https://typorasyt.oss-cn-nanjing.aliyuncs.com/202408191025655.jpg)

(上图为邢存远学长回复)

到具体代码，forward中出现的broadcast不作处理。

bp中检测到，就按照broadcast的逆流程进行压缩

```
if a.ndim!=b.dim  #make sure a.ndim>b.ndim
	把b.dim中大小为1的维度找到，并在a中的对应维度进行压缩(np.sum,keepdim=True)
	把a.dim前面多出的那一段压缩掉(理论上都是维度大小都是1，用sum还是mean不太重要)
```

## loss func

重载OP，可视为：

1.bp无需上游grad 

2.grad_func 无需传入输入

的特殊Op

## optimizer

BGD，SGD(牛魔pytorch的sgd指的不是随机梯度下降，感觉是某种梯度下降的混合体)，AdaGrad，Adam实现后进行简单比较(收敛轮数)

测试思路：调用pytorch进行收敛速度绘图和比较，尝试分析

## 一些Op

(放在nn里面)

完成func.py函数里面的函数的torch支持
