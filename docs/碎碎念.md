# MyTensor回忆录

[TOC]

## 如何实现前向连接和反向传播

前向传播的过程由用户定义的forward函数的代码决定

我们只需保证反向传播的节点索引

搞个全局list，按照forward的执行顺序将算子(node)送进去

只要倒序遍历，就一定是能合法遍历一遍的

## 梯度存哪里？进行前向传播的中间计算结果的存哪里？

我求一个函数的导数，需要它的导数表达式，和表达式中每一个参数的值，这些计算过程中产生的值怎么算？

好像有个autograd，前向传播的过程中是可以算出本地算子的梯度的，然后实际梯度按照依赖关系累乘即可

但是如果关了这个怎么办，存中间计算结果吗？如果存，存哪里？

## Broadcast

> 广播的匹配规则是：从末尾维度开始，如果两个数组的维度相同或其中一个为 1，则可以进行广播，否则会报错。

为什么是从末尾维度开始？

考虑numpy的存储，末尾维度才是最底层连续的数据，具有底层的物理意义。

尝试实现广播形状的判断

```python
import numpy as np

def Get_broadcast_shape(shape1:tuple,shape2:tuple):
    '''
    get the broadcast shape of two shapes
    '''
    if len(shape1) > len(shape2):
        shape1,shape2 = shape2,shape1
    #锁定大小，shape1<shape2
    shape1 = (1,)*(len(shape2)-len(shape1)) + shape1#把shape1,前面欠缺的维度补齐
    broadcast_shape = []
    for i in range(len(shape2)):
        if shape1[i] == shape2[i]:
            broadcast_shape.append(shape1[i])
        elif shape1[i] == 1:
            broadcast_shape.append(shape2[i])
        elif shape2[i] == 1:
            broadcast_shape.append(shape1[i])
        else:
            raise ValueError("shape not match")
    return tuple(broadcast_shape)
```



### case 1

线性层的bias是根据feature数目定的，需要对batchsize那个维度进行广播；

这个在调用np求和的时候用+而不是np.sum，np.sum基本上不具有broadcast，而使用 a+=b时请注意，b必须时broadcast的那个，否则会报错(因为相当于你对原来的那个低纬度的东西进行了维度扩展，这是高风险的)

### case 2

反向传播中的grad的求和。

#### why？

grad的初始化为data的形状，mul和sum显然形状不变(都是按对应位置的操作，等价于并行的标量级别操作)；

考虑算子matmul，考虑(a,b)@(b,c)来表示最终参与计算的维度的值

其broad_cast 规则为：

​	先进行一次普通广播，补充成一样的shape

​	1.不足二维补足二维，并且保证**b**端的维度为(m,)里面的'm'

	2. 按照物理含义，默认取最末尾的两个维度进行乘法运算。

考虑实际情况，特征一般为一维的,不会在forward的过程中出现broadcast

#### ndim<2时结果的维度变换

他的结果遵顼如下规则

c=a@b

a.shape=(m,)  $\rarr$ (,m)

b.shape=(m,) 不变

则 c.shape=()   #标量

假设我们的目标是把一维向量看成 (1,m)和(m,1)的二维向量

那么由广播导致结果会左边右边各自少一个维度，这种加减维度的行为不会影响求导过程，会等效作用于grad的维度

#### 参考资料

[NumPy的广播机制 - 邢存远的博客 | Welt Xing's Blog (welts.xyz)](https://welts.xyz/2022/04/26/broadcast/)

## SGD的实现

考虑pytorch中的sgd调用格式

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 假设我们有一个简单的线性模型
model = nn.Linear(10, 1)

# 定义损失函数
criterion = nn.MSELoss()

# 使用 SGD 优化器
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 一个训练步骤的示例
inputs = torch.randn(32, 10) 
targets = torch.randn(32, 1)  

# 前向传播
outputs = model(inputs)     
loss = criterion(outputs, targets)

# 反向传播和参数更新
optimizer.zero_grad()  
loss.backward()        
optimizer.step()      
```

这里已经对整个batch做完前传了，而且loss.backward也没考虑优化器类型，应该是全部反向传播了，那么SGD怎么选一部分样本的反向传播结果进行更新？

### 解决方案

pytorch中的sgd指的不是单样本的随机梯度下降

而是一种结合了动量法常规参数的一个优化器

### sgd的细节



![image-20240920181123636](https://typorasyt.oss-cn-nanjing.aliyuncs.com/202409201958782.png)

学长，这是您SGD的step步的代码，我不是很能理解，这里nesterov步为什么能减去grad。

这里的grad应该是基于现有参数下的梯度吗？nag不是需要data修改后的求导结果吗？

学长：俺也不知道，请翻翻手册





![image-20240920193446391](https://typorasyt.oss-cn-nanjing.aliyuncs.com/202409201958820.png)

![image-20240920193515248](https://typorasyt.oss-cn-nanjing.aliyuncs.com/202409201958704.png)

torch手册上的表述

这里式子的形式就和原来的不一样了，然后它对nesterov的处理应该是再减去一个

​	 更新后的动量*lr

根本看不懂这是什么操作(怒

这里tmd用更新后的动量来近似 nesterov要求的下一阶段的梯度，我反正理解不了，这个参数战略性放弃了
