{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:49:44.823396Z",
     "start_time": "2024-12-06T08:49:44.817292Z"
    }
   },
   "source": [
    "import sys \n",
    "import os\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 获取上级目录\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:49:46.960577Z",
     "start_time": "2024-12-06T08:49:46.885373Z"
    }
   },
   "source": [
    "#测试Exp算子\n",
    "from MyTorch import myTensor\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import numpy as np\n",
    "from MyTorch.utils.utils import myAssert\n",
    "\n",
    "for i in range(1000):\n",
    "    a=MyTensor(np.random.rand(2,3),requires_grad=True)\n",
    "    Exp=myTensor.Exp()\n",
    "    b = Exp.forward(a)\n",
    "    b.backward()\n",
    "    myAssert((b.data== a.grad).all(), \"something wrong with Exp\",b.data,a.grad)"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m Exp\u001B[38;5;241m=\u001B[39mmyTensor\u001B[38;5;241m.\u001B[39mExp()\n\u001B[0;32m     10\u001B[0m b \u001B[38;5;241m=\u001B[39m Exp\u001B[38;5;241m.\u001B[39mforward(a)\n\u001B[1;32m---> 11\u001B[0m \u001B[43mb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m myAssert((b\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m==\u001B[39m a\u001B[38;5;241m.\u001B[39mgrad)\u001B[38;5;241m.\u001B[39mall(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msomething wrong with Exp\u001B[39m\u001B[38;5;124m\"\u001B[39m,b\u001B[38;5;241m.\u001B[39mdata,a\u001B[38;5;241m.\u001B[39mgrad)\n",
      "File \u001B[1;32mE:\\myTorch\\MyTorch\\myTensor.py:233\u001B[0m, in \u001B[0;36mMyTensor.backward\u001B[1;34m(self, retain_graph)\u001B[0m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mones_like(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata)\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m op \u001B[38;5;129;01min\u001B[39;00m ComputationalGraph\u001B[38;5;241m.\u001B[39mnode_list[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 233\u001B[0m     \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mop_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;66;03m#清空自己的梯度，最终输出一定不需要梯度\u001B[39;00m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\myTorch\\MyTorch\\myTensor.py:299\u001B[0m, in \u001B[0;36mOp.op_backward\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast:\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m node\u001B[38;5;241m.\u001B[39mrequires_grad:\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;66;03m#handle broadcast\u001B[39;00m\n\u001B[1;32m--> 299\u001B[0m         add_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    300\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m node\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m!=\u001B[39madd_grad\u001B[38;5;241m.\u001B[39mshape:\n\u001B[0;32m    301\u001B[0m             \u001B[38;5;66;03m#找到被广播的维度\u001B[39;00m\n\u001B[0;32m    302\u001B[0m             broadcast_axis\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m-\u001B[39mi \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,node\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m node\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39mi]\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]     \n",
      "File \u001B[1;32mE:\\myTorch\\MyTorch\\myTensor.py:516\u001B[0m, in \u001B[0;36mMax.grad_func\u001B[1;34m(self, node, grad)\u001B[0m\n\u001B[0;32m    514\u001B[0m     y_full_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 516\u001B[0m     y_full_dim\u001B[38;5;241m=\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (np\u001B[38;5;241m.\u001B[39misclose(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdata,y_full_dim,atol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-8\u001B[39m))\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m)\u001B[38;5;241m*\u001B[39mgrad\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mexpand_dims\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\Python\\lib\\site-packages\\numpy\\lib\\shape_base.py:597\u001B[0m, in \u001B[0;36mexpand_dims\u001B[1;34m(a, axis)\u001B[0m\n\u001B[0;32m    594\u001B[0m     axis \u001B[38;5;241m=\u001B[39m (axis,)\n\u001B[0;32m    596\u001B[0m out_ndim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(axis) \u001B[38;5;241m+\u001B[39m a\u001B[38;5;241m.\u001B[39mndim\n\u001B[1;32m--> 597\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[43mnormalize_axis_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_ndim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    599\u001B[0m shape_it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(a\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    600\u001B[0m shape \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m axis \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(shape_it) \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(out_ndim)]\n",
      "File \u001B[1;32mD:\\Python\\lib\\site-packages\\numpy\\core\\numeric.py:1397\u001B[0m, in \u001B[0;36mnormalize_axis_tuple\u001B[1;34m(axis, ndim, argname, allow_duplicate)\u001B[0m\n\u001B[0;32m   1395\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1396\u001B[0m \u001B[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001B[39;00m\n\u001B[1;32m-> 1397\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([normalize_axis_index(ax, ndim, argname) \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m axis])\n\u001B[0;32m   1398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_duplicate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(axis)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(axis):\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m argname:\n",
      "File \u001B[1;32mD:\\Python\\lib\\site-packages\\numpy\\core\\numeric.py:1397\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1395\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1396\u001B[0m \u001B[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001B[39;00m\n\u001B[1;32m-> 1397\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([\u001B[43mnormalize_axis_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margname\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m axis])\n\u001B[0;32m   1398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_duplicate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(axis)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(axis):\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m argname:\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:44:40.472406Z",
     "start_time": "2024-12-06T08:44:40.457406Z"
    }
   },
   "source": [
    "#简单测试Log\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "Log=myTensor.Log()\n",
    "b = Log.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)\n",
    "print(1/a.data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.69314718 1.09861229]\n",
      " [1.38629436 1.60943791 1.79175947]]\n",
      "[[1.         0.5        0.33333333]\n",
      " [0.25       0.2        0.16666667]]\n",
      "[[1.         0.5        0.33333333]\n",
      " [0.25       0.2        0.16666667]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:44:43.671599Z",
     "start_time": "2024-12-06T08:44:43.626088Z"
    }
   },
   "source": [
    "\n",
    "#测试Log算子\n",
    "from MyTorch import MyTensor,myTensor\n",
    "import numpy as np\n",
    "from MyTorch.utils.utils import myAssert\n",
    "for i in range(1000):\n",
    "    a=MyTensor(np.random.rand(1,10),requires_grad=True)\n",
    "    Log=myTensor.Log()\n",
    "    b = Log.forward(a)\n",
    "    b.backward()\n",
    "    myAssert((a.grad==1/a.data).all(), \"something wrong with Log\",b.data,a.grad,a.data)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:44:45.387287Z",
     "start_time": "2024-12-06T08:44:45.378288Z"
    }
   },
   "source": [
    "#测试SumUnary算子\n",
    "from MyTorch.myTensor import MyTensor,SumUnary\n",
    "import numpy as np\n",
    "a=MyTensor(np.random.rand(2,3),requires_grad=True)\n",
    "Sum=SumUnary(axis=0)\n",
    "b = Sum.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.52833191 0.29345969 1.19962171]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:48:51.597074Z",
     "start_time": "2024-12-06T08:48:51.348980Z"
    }
   },
   "source": [
    "#测试div算子\n",
    "from MyTorch.myTensor import MyTensor,Div\n",
    "import numpy as np\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b=MyTensor(np.array([[2,2,2],[2,2,2]]),requires_grad=True)\n",
    "Div=Div()\n",
    "c = Div.forward(a,b)\n",
    "c.backward()\n",
    "print(c.data)\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m Max\u001B[38;5;241m=\u001B[39mMax()\n\u001B[0;32m      7\u001B[0m c \u001B[38;5;241m=\u001B[39m Max\u001B[38;5;241m.\u001B[39mforward(a)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(c\u001B[38;5;241m.\u001B[39mdata)\n",
      "File \u001B[1;32mE:\\myTorch\\MyTorch\\myTensor.py:233\u001B[0m, in \u001B[0;36mMyTensor.backward\u001B[1;34m(self, retain_graph)\u001B[0m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mones_like(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata)\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m op \u001B[38;5;129;01min\u001B[39;00m ComputationalGraph\u001B[38;5;241m.\u001B[39mnode_list[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 233\u001B[0m     \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mop_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;66;03m#清空自己的梯度，最终输出一定不需要梯度\u001B[39;00m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mE:\\myTorch\\MyTorch\\myTensor.py:299\u001B[0m, in \u001B[0;36mOp.op_backward\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast:\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m node\u001B[38;5;241m.\u001B[39mrequires_grad:\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;66;03m#handle broadcast\u001B[39;00m\n\u001B[1;32m--> 299\u001B[0m         add_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    300\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m node\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m!=\u001B[39madd_grad\u001B[38;5;241m.\u001B[39mshape:\n\u001B[0;32m    301\u001B[0m             \u001B[38;5;66;03m#找到被广播的维度\u001B[39;00m\n\u001B[0;32m    302\u001B[0m             broadcast_axis\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m-\u001B[39mi \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,node\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m node\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39mi]\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m]     \n",
      "File \u001B[1;32mE:\\myTorch\\MyTorch\\myTensor.py:516\u001B[0m, in \u001B[0;36mMax.grad_func\u001B[1;34m(self, node, grad)\u001B[0m\n\u001B[0;32m    514\u001B[0m     y_full_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 516\u001B[0m     y_full_dim\u001B[38;5;241m=\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (np\u001B[38;5;241m.\u001B[39misclose(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdata,y_full_dim,atol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-8\u001B[39m))\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m)\u001B[38;5;241m*\u001B[39mgrad\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mexpand_dims\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\Python\\lib\\site-packages\\numpy\\lib\\shape_base.py:597\u001B[0m, in \u001B[0;36mexpand_dims\u001B[1;34m(a, axis)\u001B[0m\n\u001B[0;32m    594\u001B[0m     axis \u001B[38;5;241m=\u001B[39m (axis,)\n\u001B[0;32m    596\u001B[0m out_ndim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(axis) \u001B[38;5;241m+\u001B[39m a\u001B[38;5;241m.\u001B[39mndim\n\u001B[1;32m--> 597\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[43mnormalize_axis_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_ndim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    599\u001B[0m shape_it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(a\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m    600\u001B[0m shape \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m axis \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(shape_it) \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(out_ndim)]\n",
      "File \u001B[1;32mD:\\Python\\lib\\site-packages\\numpy\\core\\numeric.py:1397\u001B[0m, in \u001B[0;36mnormalize_axis_tuple\u001B[1;34m(axis, ndim, argname, allow_duplicate)\u001B[0m\n\u001B[0;32m   1395\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1396\u001B[0m \u001B[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001B[39;00m\n\u001B[1;32m-> 1397\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([normalize_axis_index(ax, ndim, argname) \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m axis])\n\u001B[0;32m   1398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_duplicate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(axis)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(axis):\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m argname:\n",
      "File \u001B[1;32mD:\\Python\\lib\\site-packages\\numpy\\core\\numeric.py:1397\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1395\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1396\u001B[0m \u001B[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001B[39;00m\n\u001B[1;32m-> 1397\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([\u001B[43mnormalize_axis_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margname\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m axis])\n\u001B[0;32m   1398\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_duplicate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(axis)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(axis):\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m argname:\n",
      "\u001B[1;31mTypeError\u001B[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:45:07.995953Z",
     "start_time": "2024-12-06T08:44:48.452420Z"
    }
   },
   "source": [
    "#测试softmax层\n",
    "from MyTorch.my_nn import Softmax\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import torch\n",
    "import numpy as np\n",
    "from MyTorch.utils.utils import myAssert    \n",
    "for i in range(1000):\n",
    "    for dim in range(2):\n",
    "        a_np=np.random.rand(2,3)\n",
    "        a=MyTensor(a_np,requires_grad=True)\n",
    "        a_torch=torch.tensor(a_np,requires_grad=True)\n",
    "        softmax=Softmax(dim=dim)\n",
    "        b=softmax.forward(a)\n",
    "        b_torch=torch.nn.functional.softmax(a_torch,dim=dim)\n",
    "        b.backward()\n",
    "        b_torch.backward(torch.ones_like(b_torch))\n",
    "        assert np.allclose(b.data,b_torch.detach().numpy())\n",
    "        myAssert(np.allclose(a.grad,a_torch.grad.detach().numpy()),\"something wrong with softmax\",a.grad,a_torch.grad.detach().numpy())\n",
    "print(\"softmax test passed\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax test passed\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:45:15.511137Z",
     "start_time": "2024-12-06T08:45:15.501139Z"
    }
   },
   "source": [
    "#relu测试\n",
    "from MyTorch.my_nn import ReLU\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import numpy as np\n",
    "\n",
    "a=MyTensor(np.array([[-1,2,3],[4,-5,6]]),requires_grad=True)\n",
    "relu=ReLU()\n",
    "b=relu.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 3.]\n",
      " [4. 0. 6.]]\n",
      "[[0. 1. 1.]\n",
      " [1. 0. 1.]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:45:15.666642Z",
     "start_time": "2024-12-06T08:45:15.647127Z"
    }
   },
   "source": [
    "from MyTorch.my_nn import ReLU,MLP,Sequential\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import numpy as np\n",
    "\n",
    "a=MyTensor(np.array([[-1,2,3],[4,-5,6]]),requires_grad=False)\n",
    "model=Sequential(MLP(3,4,2,initial_policy='zeros'),ReLU())\n",
    "b=model.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)\n",
    "print(model.layers[0].parameters[0].grad)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "None\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Model structure:\n",
      "   Sequential(\n",
      "        MLP(3,4,2)\n",
      "        ReLU()\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:45:15.776708Z",
     "start_time": "2024-12-06T08:45:15.755132Z"
    }
   },
   "source": [
    "import sys \n",
    "import os\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 获取上级目录\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:45:17.742965Z",
     "start_time": "2024-12-06T08:45:15.857975Z"
    }
   },
   "source": [
    "import MyTorch.my_nn as nn\n",
    "import MyTorch.myTensor as MyTensor\n",
    "from MyTorch.loss_func import NLLLoss \n",
    "import numpy as np\n",
    "a=np.array([[0.1,0.2,0.3,0.4],[0.1,0.2,0.3,0.4]])\n",
    "b=np.array([1,2])\n",
    "a=MyTensor.MyTensor(a,requires_grad=True)\n",
    "b=MyTensor.MyTensor(b,requires_grad=False)\n",
    "loss=NLLLoss()\n",
    "c=loss.forward(a,b)\n",
    "c.backward()\n",
    "print(c.data)\n",
    "print(a.grad)"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLLLoss' from 'MyTorch.loss_func' (E:\\myTorch\\MyTorch\\loss_func.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mMyTorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmy_nn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mMyTorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmyTensor\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mMyTensor\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mMyTorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_func\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NLLLoss \n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      5\u001B[0m a\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray([[\u001B[38;5;241m0.1\u001B[39m,\u001B[38;5;241m0.2\u001B[39m,\u001B[38;5;241m0.3\u001B[39m,\u001B[38;5;241m0.4\u001B[39m],[\u001B[38;5;241m0.1\u001B[39m,\u001B[38;5;241m0.2\u001B[39m,\u001B[38;5;241m0.3\u001B[39m,\u001B[38;5;241m0.4\u001B[39m]])\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'NLLLoss' from 'MyTorch.loss_func' (E:\\myTorch\\MyTorch\\loss_func.py)"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.54253553 -1.44253553 -1.34253553 -1.24253553]\n",
      " [-1.54253553 -1.44253553 -1.34253553 -1.24253553]]\n"
     ]
    }
   ],
   "source": [
    "#测试logsoftmax\n",
    "import MyTorch.my_nn as nn\n",
    "import MyTorch.my_nn as nn\n",
    "import MyTorch.myTensor as MyTensor\n",
    "from MyTorch.loss_func import NLLLoss \n",
    "import numpy as np\n",
    "a=np.array([[0.1,0.2,0.3,0.4],[0.1,0.2,0.3,0.4]])\n",
    "b=np.array([1,2])\n",
    "a=MyTensor.MyTensor(a,requires_grad=True)\n",
    "b=MyTensor.MyTensor(b,requires_grad=False)\n",
    "logsoftmax=nn.LogSoftmax(dim=1)\n",
    "c=logsoftmax.forward(a)\n",
    "print(c.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.         -0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "#test mseloss2\n",
    "#forward result test\n",
    "import numpy as np\n",
    "from MyTorch.myTensor import MyTensor\n",
    "from MyTorch.loss_func import MSEloss2\n",
    "y_true=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=False)\n",
    "y_pred=MyTensor(np.array([[1,2,3],[4,5,4]]),requires_grad=True)\n",
    "criterion=MSEloss2()\n",
    "result=criterion(y_pred,y_true)\n",
    "print(result.data)\n",
    "#backward test\n",
    "result.backward()\n",
    "print(y_pred.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5514447139320511\n",
      "[[ 0.21194156 -0.10597078 -0.10597078]\n",
      " [-0.10597078  0.21194156 -0.10597078]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001B[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001B[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001B[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "#test crossentropyloss2\n",
    "#forward result test\n",
    "import numpy as np\n",
    "from MyTorch.myTensor import MyTensor\n",
    "from MyTorch.loss_func import CrossEntropyLoss2\n",
    "y_true=MyTensor(np.array([[1,0,0],[0,1,0]]),requires_grad=False)\n",
    "y_pred=MyTensor(np.array([[1,0,0],[0,1,0]]),requires_grad=True)\n",
    "criterion=CrossEntropyLoss2()\n",
    "result=criterion(y_pred,y_true)\n",
    "print(result.data)\n",
    "#backward test\n",
    "result.backward()\n",
    "print(y_pred.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
