{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 获取上级目录\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试Exp算子\n",
    "from MyTorch import myTensor\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import numpy as np\n",
    "from MyTorch.utils.utils import myAssert\n",
    "\n",
    "for i in range(1000):\n",
    "    a=MyTensor(np.random.rand(2,3),requires_grad=True)\n",
    "    Exp=myTensor.Exp()\n",
    "    b = Exp.forward(a)\n",
    "    b.backward()\n",
    "    myAssert((b.data== a.grad).all(), \"something wrong with Exp\",b.data,a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.69314718 1.09861229]\n",
      " [1.38629436 1.60943791 1.79175947]]\n",
      "[[1.         0.5        0.33333333]\n",
      " [0.25       0.2        0.16666667]]\n",
      "[[1.         0.5        0.33333333]\n",
      " [0.25       0.2        0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "#简单测试Log\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "Log=myTensor.Log()\n",
    "b = Log.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)\n",
    "print(1/a.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#测试Log算子\n",
    "from MyTorch import MyTensor,myTensor\n",
    "import numpy as np\n",
    "from MyTorch.utils.utils import myAssert\n",
    "for i in range(1000):\n",
    "    a=MyTensor(np.random.rand(1,10),requires_grad=True)\n",
    "    Log=myTensor.Log()\n",
    "    b = Log.forward(a)\n",
    "    b.backward()\n",
    "    myAssert((a.grad==1/a.data).all(), \"something wrong with Log\",b.data,a.grad,a.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93408124 0.36217092 1.09866222]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#测试SumUnary算子\n",
    "from MyTorch.myTensor import MyTensor,SumUnary\n",
    "import numpy as np\n",
    "a=MyTensor(np.random.rand(2,3),requires_grad=True)\n",
    "Sum=SumUnary(axis=0)\n",
    "b = Sum.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1.  1.5]\n",
      " [2.  2.5 3. ]]\n",
      "[[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n",
      "[[-0.25 -0.5  -0.75]\n",
      " [-1.   -1.25 -1.5 ]]\n"
     ]
    }
   ],
   "source": [
    "#测试div算子\n",
    "from MyTorch.myTensor import MyTensor,Div\n",
    "import numpy as np\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b=MyTensor(np.array([[2,2,2],[2,2,2]]),requires_grad=True)\n",
    "Div=Div()\n",
    "c = Div.forward(a,b)\n",
    "c.backward()\n",
    "print(c.data)\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax test passed\n"
     ]
    }
   ],
   "source": [
    "#测试softmax层\n",
    "from MyTorch.my_nn import Softmax\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import torch\n",
    "import numpy as np\n",
    "from MyTorch.utils.utils import myAssert    \n",
    "for i in range(1000):\n",
    "    for dim in range(2):\n",
    "        a_np=np.random.rand(2,3)\n",
    "        a=MyTensor(a_np,requires_grad=True)\n",
    "        a_torch=torch.tensor(a_np,requires_grad=True)\n",
    "        softmax=Softmax(dim=dim)\n",
    "        b=softmax.forward(a)\n",
    "        b_torch=torch.nn.functional.softmax(a_torch,dim=dim)\n",
    "        b.backward()\n",
    "        b_torch.backward(torch.ones_like(b_torch))\n",
    "        assert np.allclose(b.data,b_torch.detach().numpy())\n",
    "        myAssert(np.allclose(a.grad,a_torch.grad.detach().numpy()),\"something wrong with softmax\",a.grad,a_torch.grad.detach().numpy())\n",
    "print(\"softmax test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 3.]\n",
      " [4. 0. 6.]]\n",
      "[[0. 1. 1.]\n",
      " [1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#relu测试\n",
    "from MyTorch.my_nn import ReLU\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import numpy as np\n",
    "\n",
    "a=MyTensor(np.array([[-1,2,3],[4,-5,6]]),requires_grad=True)\n",
    "relu=ReLU()\n",
    "b=relu.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "None\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Model structure:\n",
      "   Sequential(\n",
      "        MLP(3,4,2)\n",
      "        ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from MyTorch.my_nn import ReLU,MLP,Sequential\n",
    "from MyTorch.myTensor import MyTensor\n",
    "import numpy as np\n",
    "\n",
    "a=MyTensor(np.array([[-1,2,3],[4,-5,6]]),requires_grad=False)\n",
    "model=Sequential(MLP(3,4,2,initial_policy='zeros'),ReLU())\n",
    "b=model.forward(a)\n",
    "b.backward()\n",
    "print(b.data)\n",
    "print(a.grad)\n",
    "print(model.layers[0].parameters[0].grad)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 获取上级目录\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true is not one-hot, converting to one-hot...\n",
      "1.9560115027140728\n",
      "[[-5.   0.   0.   0. ]\n",
      " [ 0.  -2.5  0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "import MyTorch.my_nn as nn\n",
    "import MyTorch.myTensor as MyTensor\n",
    "from MyTorch.loss_func import NLLLoss \n",
    "import numpy as np\n",
    "a=np.array([[0.1,0.2,0.3,0.4],[0.1,0.2,0.3,0.4]])\n",
    "b=np.array([1,2])\n",
    "a=MyTensor.MyTensor(a,requires_grad=True)\n",
    "b=MyTensor.MyTensor(b,requires_grad=False)\n",
    "loss=NLLLoss()\n",
    "c=loss.forward(a,b)\n",
    "c.backward()\n",
    "print(c.data)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.54253553 -1.44253553 -1.34253553 -1.24253553]\n",
      " [-1.54253553 -1.44253553 -1.34253553 -1.24253553]]\n"
     ]
    }
   ],
   "source": [
    "#测试logsoftmax\n",
    "import MyTorch.my_nn as nn\n",
    "import MyTorch.my_nn as nn\n",
    "import MyTorch.myTensor as MyTensor\n",
    "from MyTorch.loss_func import NLLLoss \n",
    "import numpy as np\n",
    "a=np.array([[0.1,0.2,0.3,0.4],[0.1,0.2,0.3,0.4]])\n",
    "b=np.array([1,2])\n",
    "a=MyTensor.MyTensor(a,requires_grad=True)\n",
    "b=MyTensor.MyTensor(b,requires_grad=False)\n",
    "logsoftmax=nn.LogSoftmax(dim=1)\n",
    "c=logsoftmax.forward(a)\n",
    "print(c.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.         -0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "#test mseloss2\n",
    "#forward result test\n",
    "import numpy as np\n",
    "from MyTorch.myTensor import MyTensor\n",
    "from MyTorch.loss_func import MSEloss2\n",
    "y_true=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=False)\n",
    "y_pred=MyTensor(np.array([[1,2,3],[4,5,4]]),requires_grad=True)\n",
    "criterion=MSEloss2()\n",
    "result=criterion(y_pred,y_true)\n",
    "print(result.data)\n",
    "#backward test\n",
    "result.backward()\n",
    "print(y_pred.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5514447139320511\n",
      "[[ 0.21194156 -0.10597078 -0.10597078]\n",
      " [-0.10597078  0.21194156 -0.10597078]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "#test crossentropyloss2\n",
    "#forward result test\n",
    "import numpy as np\n",
    "from MyTorch.myTensor import MyTensor\n",
    "from MyTorch.loss_func import CrossEntropyLoss2\n",
    "y_true=MyTensor(np.array([[1,0,0],[0,1,0]]),requires_grad=False)\n",
    "y_pred=MyTensor(np.array([[1,0,0],[0,1,0]]),requires_grad=True)\n",
    "criterion=CrossEntropyLoss2()\n",
    "result=criterion(y_pred,y_true)\n",
    "print(result.data)\n",
    "#backward test\n",
    "result.backward()\n",
    "print(y_pred.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
