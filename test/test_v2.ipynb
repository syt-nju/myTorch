{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 获取上级目录\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from MyTorch_v2.myTensor import MyTensor,Sum\n",
    "import numpy as np\n",
    "#构造两个2*3的张量\n",
    "a = MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b= MyTensor(np.array([[1,2,3],[4,5,6]]))\n",
    "c=Sum.forward(a,b)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "#test for sub\n",
    "from MyTorch_v2.myTensor import Sub\n",
    "a = MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b= MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "c=Sub.forward(a,b)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  4.  9.]\n",
      " [ 4. 10. 18.]]\n",
      "[[1. 2. 3.]\n",
      " [1. 2. 3.]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "#test for mul\n",
    "from MyTorch_v2.myTensor import Mul\n",
    "a = MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b= MyTensor(np.array([[1,2,3],[1,2,3]]),requires_grad=True)\n",
    "c=Mul.forward(a,b)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]]\n",
      "[[ 3.  7. 11.]]\n",
      "[[1. 1.]\n",
      " [2. 2.]\n",
      " [3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "#test for matmul\n",
    "from MyTorch_v2.myTensor import MatMul\n",
    "# a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "# b=MyTensor(np.array([[1,2],[3,4],[5,6]]),requires_grad=True)\n",
    "# c=MatMul.forward(a,b)\n",
    "# # print(c.data)\n",
    "# c.backward()\n",
    "# # print(a.grad)\n",
    "# # print(b.grad)\n",
    "\n",
    "#test for one_dim_matmul\n",
    "a=MyTensor(np.array([[1,2,3]]),requires_grad=True)\n",
    "b=MyTensor(np.array([[1,2],[3,4],[5,6]]),requires_grad=True)\n",
    "c=MatMul.forward(a,b)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[1.         0.5        0.33333333]\n",
      " [0.25       0.2        0.16666667]]\n",
      "[[-1.         -0.5        -0.33333333]\n",
      " [-0.25       -0.2        -0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "#test for div\n",
    "from MyTorch_v2.myTensor import Div\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "c=Div.forward(a,b)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 6.]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#test for max\n",
    "from MyTorch_v2.myTensor import Max\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "c=Max.forward(a,axis=1,keepdims=False)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.71828183   7.3890561   20.08553692]\n",
      " [ 54.59815003 148.4131591  403.42879349]]\n",
      "[[  2.71828183   7.3890561   20.08553692]\n",
      " [ 54.59815003 148.4131591  403.42879349]]\n",
      "[[  2.71828183   7.3890561   20.08553692]\n",
      " [ 54.59815003 148.4131591  403.42879349]]\n",
      "[[1.36787944 1.13533528 1.04978707]\n",
      " [1.01831564 1.00673795 1.00247875]]\n"
     ]
    }
   ],
   "source": [
    "#test for exp log\n",
    "from MyTorch_v2.myTensor import Exp,Log\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "c=Exp.forward(a)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "d=Log.forward(c)\n",
    "print(c.data)\n",
    "d.backward()\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6. 15.]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#test for sumunary\n",
    "from MyTorch_v2.myTensor import SumUnary\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "c=SumUnary.forward(a,axis=1,keepdims=False)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[0. 0. 0.]\n",
      " [0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from MyTorch_v2.loss_func import MSELoss\n",
    "from MyTorch_v2.myTensor import MyTensor\n",
    "a=MyTensor(np.array([[1,2,3],[4,5,6]]),requires_grad=True)\n",
    "b=MyTensor(np.array([[1,2,3],[4,4,6]]),requires_grad=False)\n",
    "criterion=MSELoss()\n",
    "c=criterion.forward(a,b)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #a small test\n",
    "# #y=3x+9\n",
    "# from MyTorch_v2.myTensor import MyTensor\n",
    "# from MyTorch_v2.my_nn import MyLinearLayer,Sequential,ReLU\n",
    "# from MyTorch_v2.loss_func import MSELoss\n",
    "# from MyTorch_v2.optim import Adam\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# # 创建 TensorBoard 日志记录器\n",
    "# writer = SummaryWriter('runs/gradient_tracking')\n",
    "# model=MyLinearLayer(2,2,initial_policy=\"zeros\")\n",
    "# criterion=MSELoss()\n",
    "# optimizer=Adam(model.parameters)\n",
    "\n",
    "# epochs=10000\n",
    "# for i in range(epochs):\n",
    "#     input_np=np.random.randn(2)\n",
    "#     # input_np=np.array([[1,2]])\n",
    "#     target_np=0.5*input_np+0.3\n",
    "#     input=MyTensor(input_np)\n",
    "#     output=model.forward(input)\n",
    "#     target=MyTensor(target_np)\n",
    "#     loss=criterion.forward(output,target)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(loss.data)\n",
    "#     writer.add_histogram(\"weight\",model.parameters[0].data,i)\n",
    "#     writer.add_histogram(\"bias\",model.parameters[1].data,i)\n",
    "#     writer.add_histogram(\"loss\",loss.data,i)\n",
    "# print(model(MyTensor(np.array([[0.5,-0.6]]))).data)\n",
    "# writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true is not one-hot, converting to one-hot...\n",
      "-1.9077806097480852\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m c\u001b[38;5;241m=\u001b[39mcriterion\u001b[38;5;241m.\u001b[39mforward(y_pred,y_true)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(c\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32mc:\\Users\\nju22\\Desktop\\myTorch\\MyTorch_v2\\myTensor.py:205\u001b[0m, in \u001b[0;36mMyTensor.backward\u001b[1;34m(self, retain_graph)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mfather_tensor:\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m--> 205\u001b[0m                 \u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfather_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfather_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m#清空计算图\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retain_graph:\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,3)"
     ]
    }
   ],
   "source": [
    "#cross entropy loss 和 nll loss 的可跑性测试\n",
    "from MyTorch_v2.loss_func import CrossEntropyLoss,NLLLoss\n",
    "y_pred=MyTensor(np.array([[0.1,0.2,0.7],[0.2,0.3,0.5]]),requires_grad=True)\n",
    "y_true=MyTensor(np.array([2,1]),requires_grad=False)\n",
    "criterion=CrossEntropyLoss()\n",
    "c=criterion.forward(y_pred,y_true)\n",
    "print(c.data)\n",
    "c.backward()\n",
    "print(y_pred.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
