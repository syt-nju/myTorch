# 常见优化器与比较

**开摆了，看参考资料，学长写的太好了**

优化器的行为本质上就是把所有需要更新的参数-=grad*lr

不同优化器基本上都是采用不同的对lr的调整策略

我们需要区分优化器之间的使用情况和各自的特点



## BGD

对一个batch，固定学习率的梯度下降

## AdaGrad

### 特点

自适应调节lr参数，调用的时候一般使用默认的lr值

对高频更新参数采用比较小的lr，对比较大的采用比较大的lr

### 公式

$$
\theta_{t+1,i}=\theta_{t,i}-\dfrac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\cdot g_{t,i}
$$

$G_{t,ii}$指该参数之前的grad的平方和

### 不足

随着轮次增多lr会趋于零



## 重要参考资料

[An overview of gradient descent optimization algorithms - 邢存远的博客 | Welt Xing's Blog (welts.xyz)](https://xingcy.net/2021/08/20/gd/)